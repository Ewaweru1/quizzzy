{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mcq-gen.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiur3GltG/BBy/ZqKpm2yk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthick47v2/mcq-generator/blob/base-dev/mcq_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn0e31wBJPRh"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers==4.15.0    # only this ver works with FastT5\n",
        "!pip3 install SentencePiece\n",
        "!pip3 install git+https://github.com/boudinfl/pke.git\n",
        "!pip3 install fastt5\n",
        "\n",
        "!pip install keybert\n",
        "!pip3 install keyphrase-vectorizers\n",
        "\n",
        "!pip install sense2vec\n",
        "!pip install sentence_transformers==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## IMPORT LIBS\n",
        "# natural language toolkit for helping utilities\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# text preprocessing\n",
        "import string\n",
        "import re\n",
        "\n",
        "# meaningful keyword extraction\n",
        "# https://github.com/MaartenGr/KeyBERT\n",
        "# https://github.com/TimSchopf/KeyphraseVectorizers\n",
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "\n",
        "# onnx model inferece \n",
        "# https://github.com/Ki6an/fastT5\n",
        "from fastT5 import get_onnx_model, get_onnx_runtime_sessions, OnnxT5 \n",
        "\n",
        "# working with transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# helper\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# file dir helper\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# generate simialr words\n",
        "# https://github.com/explosion/sense2vec\n",
        "from sense2vec import Sense2Vec\n",
        "\n",
        "# sentence embedding\n",
        "# https://www.sbert.net/\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# commonly used model\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
        "\n",
        "# word similiarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "dVzDR6qRJdFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# we are using 2015 reddit comments --- better than 2019\n",
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xf s2v_reddit_2015_md.tar.gz"
      ],
      "metadata": {
        "id": "dHyx_MUWJlYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## INITIALIZE ALL REQUIRED MODELS\n",
        "# load vectors\n",
        "s2v = Sense2Vec().from_disk(\"/content/s2v_old\")\n",
        "\n",
        "# initialize keyword extration model (KeyBERT) and keypharse vectorizer for meaningful keywords\n",
        "kw_model = KeyBERT()\n",
        "vectorizer = KeyphraseCountVectorizer()\n",
        "\n",
        "# initialize summarize model\n",
        "model_path = '/content/gdrive/MyDrive/mcq-gen/t5-base'\n",
        "model_name = \"t5-base\"\n",
        "encoder_path = os.path.join(model_path, f\"{model_name}-encoder-quantized.onnx\")\n",
        "decoder_path = os.path.join(model_path, f\"{model_name}-decoder-quantized.onnx\")\n",
        "init_decoder_path = os.path.join(model_path, f\"{model_name}-init-decoder-quantized.onnx\")\n",
        "\n",
        "model_sessions = get_onnx_runtime_sessions((encoder_path,decoder_path,init_decoder_path))\n",
        "sum_model = OnnxT5(model_path, model_sessions)\n",
        "sum_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# initialize question generation model\n",
        "model_path = '/content/gdrive/MyDrive/mcq-gen/t5-question'\n",
        "model_name = 't5-question'\n",
        "encoder_path = os.path.join(model_path, f\"{model_name}-encoder-quantized.onnx\")\n",
        "decoder_path = os.path.join(model_path, f\"{model_name}-decoder-quantized.onnx\")\n",
        "init_decoder_path = os.path.join(model_path, f\"{model_name}-init-decoder-quantized.onnx\")\n",
        "\n",
        "model_sessions = get_onnx_runtime_sessions((encoder_path,decoder_path,init_decoder_path))\n",
        "q_model = OnnxT5(model_path, model_sessions)\n",
        "q_tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "L3V19jO2JssZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCIONS FOR SUMMARIZATION\n",
        "# preprocess the text (removing unwanted signs)\n",
        "# remove all signs other than -,-,a-z,A-Z,0-9..... and remove all extra blank spaces\n",
        "def preprocess_bulk_text(text):\n",
        "  text = text.strip()\n",
        "  text = re.sub('[\\u2010-\\u2013]', '-', text)\n",
        "  text = re.sub('[^a-zA-Z0-9\\.,-?%&*()]', ' ', text)\n",
        "  text = re.sub(' {2,}', ' ', text)\n",
        "  return text\n",
        "\n",
        "# split the bulk input text into required input length for summarizing model\n",
        "def split_text(text, range=300):\n",
        "  bulk_text = preprocess_bulk_text(text)\n",
        "  splitted_texts = []\n",
        "  # split whole input into $(range) block of meaningful text. (only split after a full stop)\n",
        "  while(len(bulk_text) > range):\n",
        "    i = range\n",
        "    while((i < len(bulk_text)) and (bulk_text[i] != '.')):\n",
        "      i += 1\n",
        "    splitted_texts.append(bulk_text[:(i+1)])\n",
        "    bulk_text = bulk_text.replace(bulk_text[:(i+1)], \"\")\n",
        "  return splitted_texts\n",
        "\n",
        "# preprocess splitted text to required input format for summarizer model\n",
        "def preprocess_splitted_text(text):\n",
        "  # \"summarize: xxxxxxxx\" is the input format for model\n",
        "  encode = sum_tokenizer.encode_plus(\"summarize: \" + text, return_tensors='pt', pad_to_max_length=False, truncation=True)\n",
        "  return encode[\"input_ids\"], encode[\"attention_mask\"]\n",
        "\n",
        "# summarize input text\n",
        "def summarize(text):\n",
        "  input_tokens_ids, attention_mask = preprocess_splitted_text(text)\n",
        "  # encoded output\n",
        "  summary_encoded = sum_model.generate(input_ids=input_tokens_ids, \n",
        "                                   attention_mask=attention_mask,\n",
        "                                   num_beams=3,                       # get the sentence with max prob of 3 tokens\n",
        "                                   num_return_sequences=1,            # only need 1 outpu\n",
        "                                   no_repeat_ngram_size=2,            # no repeat of 2 ngram\n",
        "                                   max_length=512,                    # model's in length - default\n",
        "                                   early_stopping=True)\n",
        "  \n",
        "\n",
        "  # decode summarized token\n",
        "  output = sum_tokenizer.decode(summary_encoded[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "  return postprocess_summary(output)\n",
        "\n",
        "# postpress the output of summarizer model for fair readable output\n",
        "# capitalize firt word of sentence. put spaces in required place\n",
        "def postprocess_summary(text):\n",
        "  output = \"\"\n",
        "\n",
        "  for x in sent_tokenize(text):\n",
        "    x = x.capitalize()\n",
        "    output += \" \" + x\n",
        "  return output"
      ],
      "metadata": {
        "id": "5Un6kBdMJ4Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCTIONS FOR KEYWORD EXTRACTION\n",
        "# extract keywords using KeyBERT\n",
        "def extract_keywords(text, kw_pop):\n",
        "  kw = kw_model.extract_keywords(text, vectorizer=vectorizer)\n",
        "\n",
        "  kw_ls = []\n",
        "  for i in kw:\n",
        "    # 0 -> keyword, 1-> confidence / probability\n",
        "    kw_ls.append(i[0])\n",
        "  return kw_ls\n",
        "\n",
        "# extract keywords from both summary and original text and only \n",
        "# return keywords which are common (extra validation)\n",
        "\n",
        "# max keywords per summary-original pair is 5 so that we can reduce \n",
        "# unnecessary extra questions\n",
        "def filter_keywords(original, summarized, kw_pop=5):\n",
        "  orig_ls = extract_keywords(original, kw_pop)\n",
        "  sum_ls = extract_keywords(summarized, kw_pop)\n",
        "  orig_ls = set(orig_ls)\n",
        "  return list(orig_ls.intersection(sum_ls))"
      ],
      "metadata": {
        "id": "xjHzUowXJ9j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCTIONS FOR FALSE ANSWERS\n",
        "# generate false answers from correct answer \n",
        "def false_answers(query, word_similarity_threshold=0.7):\n",
        "  # get the best sense for given word (like NOUN, PRONOUN, VERB...)\n",
        "  query_al = s2v.get_best_sense(query.lower().replace(' ', '_'))\n",
        "\n",
        "  # sometimes word won't be in sense2vec in that case we can't produce any output -- ##### TODO DO: DROP THAT QUESTION\n",
        "  try:\n",
        "    assert query_al in s2v\n",
        "    # get most similar 20 words (if any)\n",
        "    temp = s2v.most_similar(query_al, n=20)\n",
        "    formatted_string = change_format(query_al, temp)\n",
        "    formatted_string.insert(0, query)\n",
        "    # if answers are numbers then we don't need to filter \n",
        "    if query_al.split('|')[1] == 'CARDINAL':\n",
        "      return formatted_string[:4]\n",
        "    # else filter because sometimes similar words will be US, U.S, USA, AMERICA.. bt all are same no?\n",
        "    return filter_output(query, formatted_string)\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "# change s2v format to fair readable form\n",
        "def change_format(query, distractors):\n",
        "  output = []\n",
        "  for result in distractors:\n",
        "    res = result[0].split('|')\n",
        "    res = res[0].replace('_', ' ')\n",
        "    res = res[0].upper() + res[1:]\n",
        "    output.append(res)\n",
        "  return output\n",
        "\n",
        "# generate embeddings \n",
        "def return_embedding(answer, distractors):\n",
        "  return sentence_model.encode([answer]), sentence_model.encode(distractors)\n",
        "\n",
        "# filter false answers \n",
        "def filter_output(orig, dummies):\n",
        "  ans_embedded, dis_embedded = return_embedding(orig, dummies)\n",
        "  # filter using MMMR \n",
        "  dist = mmr(ans_embedded, dis_embedded,dummies)\n",
        "\n",
        "  filtered_dist = []\n",
        "  for d in dist:\n",
        "    # 0 -> word, 1 -> confidence / probability\n",
        "    filtered_dist.append(d[0])\n",
        "\n",
        "  return filtered_dist\n",
        "\n",
        "# Mdicersity using MR - Maximal Marginal Relevence\n",
        "def mmr(doc_embedding, word_embedding, words, top_n=4, diversity=0.9):\n",
        "  # extract similarity between words and docs\n",
        "  word_doc_similarity = cosine_similarity(word_embedding, doc_embedding)\n",
        "  word_similarity = cosine_similarity(word_embedding)\n",
        "  \n",
        "  kw_idx = [np.argmax(word_doc_similarity)]\n",
        "  dist_idx = [i for i in range(len(words)) if i != kw_idx[0]]\n",
        "\n",
        "  for i in range(top_n - 1):\n",
        "    dist_similarities = word_doc_similarity[dist_idx, :]\n",
        "    target_similarities = np.max(word_similarity[dist_idx][:, kw_idx], axis=1)\n",
        "\n",
        "    # calculate MMR\n",
        "    mmr = (1 - diversity) * dist_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "    mmr_idx = dist_idx[np.argmax(mmr)]\n",
        "\n",
        "    # update kw\n",
        "    kw_idx.append(mmr_idx)\n",
        "    dist_idx.remove(mmr_idx)\n",
        "\n",
        "  return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in kw_idx]"
      ],
      "metadata": {
        "id": "j5prglqlKG6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## HELPER FUNCTIONS FOR QUESTION GENERATION\n",
        "# preprocess the summary for question generation\n",
        "def preprocess_summary(context, answer):\n",
        "  # \"context: XXXXXXXXxxxxx answer: XXXXXXXXX\" is the required format for question generation model\n",
        "  text = \"context: {} answer: {}\".format(context, answer)\n",
        "  encode = q_tokenizer.encode_plus(text, \n",
        "                                   return_tensors='pt',\n",
        "                                   max_length = 382,                  # for meaningful context-question pair ---- no a magical number\n",
        "                                   pad_to_max_length=False,      \n",
        "                                   truncation=True)\n",
        "  return encode[\"input_ids\"], encode[\"attention_mask\"]\n",
        "\n",
        "# generate questions from context-answer pair\n",
        "def gen_question(context, answer):\n",
        "  input_tokens_ids, attention_mask = preprocess_summary(context, answer)\n",
        "\n",
        "  # encoded output\n",
        "  question_encoded = q_model.generate(input_ids=input_tokens_ids, \n",
        "                                             attention_mask=attention_mask,\n",
        "                                             num_beams=5,             # 5 gave good results \n",
        "                                             no_repeat_ngram_size=2,  #\n",
        "                                             max_length=72,           # single question's max token len-- just an arbitary no -- but its enough\n",
        "                                             early_stopping=True)\n",
        "  \n",
        "  # decode summarized token and post process it before print\n",
        "  output = q_tokenizer.decode(question_encoded[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "  output = output.replace(\"question: \", \"\")\n",
        "  output = output.strip()\n",
        "  return output"
      ],
      "metadata": {
        "id": "26pCANW0KLQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = \"\"\"\n",
        "Manufacturing processes are the steps through which raw materials are transformed into a final product. The manufacturing process begins with the use of the materials and then modified through manufacturing processes to become the required part. The process involves use of machinery, tools, power and labour. During the process, it adds greater valve to the final product. Therefore, manufacturing is a value added process.\n",
        "Machinery Tools Power Labour\n",
        "Raw materials Product\n",
        "Raw materials: Raw materials are often natural resources such as crude oil, iron ore and wood. They are harvested from the earth. Processed materials are materials refined by humans.eg steel, petrol, paper, glass Iron ore: Iron ore is available on earth in the form of rocks rich in iron oxide. It also contains other impurities such as Sulphur. Iron is extracted from iron ore by heating them (above 1250 deg C) with coke (Coke is a refined form of coal) in a furnace called blast furnace. Oxides and other impurities are removed from the iron ore to leave the iron behind which is called ‘Pig iron’.\n",
        "Pig iron ingots Pig iron contains higher percentage of carbon (3.5 to 4.5%). Therefore, it is very hard and brittle and it cannot be used as an engineering material. To become steel, it must be melted again and reprocessed to reduce the carbon to the correct amount. Steel is iron and up to 1.5% carbon. Elements such as chromium, nickel, tungsten, vanadium can also be added to steel, to get different properties. When added, they are called alloy steels. After preparation of the correct material, the liquid steel is allowed to solidify in the form of a billet, slab or an ingot. These are again processed to make final products.eg. Sheets, tubes, rods This includes production of iron, copper, aluminum and other metals from their ores.\n",
        "The basic properties of materials\n",
        "1. Physical properties:\n",
        "These can be considered to include density, specific gravity and melting point.\n",
        "2. Electrical properties:\n",
        "Electrical properties are resistivity and conductivity.\n",
        "3. Thermal properties:\n",
        "These are displayed when there is heat input to a material and include expansion, thermal conductivity and specific heat.\n",
        "4. Chemical properties:\n",
        "These are, for example, corrosion\n",
        "5. Mechanical properties:\n",
        "The mechanical properties of materials defined the behavior of materials under the action of external forces called loads or stresses. There is a measure of strength and durability of a material in service. These properties are great importance in the design of machines and structures.\n",
        "The most important mechanical properties\n",
        "1. Strength\n",
        "The strength of material is its capacity to withstand failure under the action of external\n",
        "loads. The stronger the material the greater the load it can withstand before failure.\n",
        "Types of Stresses: -\n",
        "(i). Tensile Stress: the force acts to pull materials apart\n",
        "(ii). Compressive Stress: the force squeezes material\n",
        "(iii). Shear Stress: the force causes one part to slide on another part\n",
        "2. Elasticity\n",
        "The elasticity of a metal is its power of coming back to its original shape after deformation when the loads are removed.\n",
        "3. Plasticity\n",
        "The plasticity of a metal is the ability to change its shape without destruction under the application of loads, and to retain its shape, when the loads are withdrawn.\n",
        "4. Toughness\n",
        "This ability of a material to resist hammering or impact loads without fracturing. Toughness is a high desirable quality for structural and machine parts which have to withstand shocks and vibrations.\n",
        "5. Brittleness\n",
        "The brittleness of material is the property of braking without much permanent distortion. These materials break into pieces due to impact.\n",
        "6. Malleability\n",
        "The malleability of metal is its ability to change shape by external force (hammering) without breaking.\n",
        "7. Ductility\n",
        "The ductility of a metal is the property which enables a metal to be drawn into wires without breaking. Copper is a ductile material. The ductility of a material increases with the temperature.\n",
        "8. Hardness\n",
        "This is ability of a material to resist wear. (eg. knife, file). Hard materials can be used to cut soft materials.\n",
        "Classification of engineering materials\n",
        "Following chart gives a classification of engineering materials that are commonly used in a\n",
        "Workshop\n",
        "Plain carbon steel: Carbon steel is the most widely used kind of steel. The properties of carbon steel depend primarily on the amount of carbon it contains. There are three types of plain carbon steels. (a) Mild steel (up to 0.3% C) (b) Medium carbon steel (0.3 to 0.8%) (c) High carbon steel (0.8 to 1.5%) The following graph shows the variation of (i) strength (toughness), (ii) ductility and malleability, (iii) hardness & brittleness with increase in carbon percentage.\n",
        "Cast iron\n",
        "Cast iron is made by melting pig iron with cast iron scrap and steel scrap in cupola furnaces and poured into molds to make castings. Cast Iron is generally defined as an alloy of iron with 2.5 % to 3 5 % Carbon, and usually with small amount of Silicon and Manganese. Due to its high carbon percentage that exists in the form of graphite, it has self lubricating property.\n",
        "Cast iron is comparatively weak and brittle in tension and has a high hardness. It can take high compression loads. It has lubricating properties and easily cast. (casting is a manufacturing process)\n",
        "Cast iron is used to make automotive parts such as engine blocks, cylinder liners, cylinder heads, machine parts, industrial components, pump housings and motor housings. Applications of mild steel, medium carbon steel, high carbon steel and cast iron are given below.\n",
        "Alloy steels: Steel has Iron and Carbon. Alloy steel has one or more alloying elements other than iron and carbon such as chromium, vanadium, nickel, tungsten etc. The basic properties of steel given above in this chapter can be changed by adding various alloying elements. Applications of alloy steels:- (a) Stainless steel: - Good quality stainless steel has about 18 to 20% Chromium as the alloying element. Stainless steel does not readily corrode, rust or stain with water as ordinary steel does. It has the ability to resist oxidation.\n",
        "Stainless steel sheet Cutlery setWrist watch strap\n",
        "Medical equipment Kitchen items Tubes and sections\n",
        "(b) High speed steel (HSS):-\n",
        "High Speed Steel contains about 18% Tungsten as the alloying element. It performs a high hardness and a high wear resistance. Therefore, this material is used to make cutting tools (tools to cut other materials of less hardness).\n",
        "(c) Vanadium steel:-\n",
        "Vanadium has the ability to improve the strength and toughness of steel. Therefore, it is used to make tools which need high strength.\n",
        "Chromium – Vanadium steel spanner\n",
        "Non ferrous metals:-\n",
        "Non-ferrous metals do not contain iron (Fe) in appreciable amounts. They are more expensive than ferrous metals. They have exceptional engineering properties.\n",
        "Example of some non ferrous metals used in the industry: • Aluminum has low weight and resistance to corrosion. • Copper has high electrical and thermal conductivity and resistance to corrosion. Electrical wires (conductors) are made of copper • Lead is a good conductor of electricity. Lead and Tin are mixed 1 : 1 ratio to make solder. • Tin is applied on ferrous metals (inside cans) to prevent corrosion • Zinc has high ability to resist corrosion. Melted zinc is applied on ferrous metals to prevent corrosion. The process is called galvanizing.\n",
        "Non ferrous alloys:-\n",
        "(a) Aluminum that is used in the industry is not pure aluminum, but they are alloys. Alloying elements are copper, silicon, tin, zinc etc. Their applications are automobile bodies, aircrafts, cans, alloy wheels, tubes and sections and sheets.\n",
        "(b) Applications of Copper alloys:\n",
        "Brass and bronze are copper alloys. Brass (copper and zinc) and bronze (copper and tin).\n",
        "Brass and bronze are used to make water and gas taps, pipes, bushes, door hinges and\n",
        "door locks, household items, and statues.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "## MAIN FUNCTIONS\n",
        "# generate questions from keywords\n",
        "splitted_text = split_text(full_text)\n",
        "\n",
        "summary = []\n",
        "filtered_kw = []\n",
        "questions_w_ans = []\n",
        "all_answers = []\n",
        "\n",
        "# summarize and find keywords for each splitted text\n",
        "for i in range(len(splitted_text)):\n",
        "  summary.append(summarize(splitted_text[i]))\n",
        "  filtered_kw.append(filter_keywords(splitted_text[i], summary[i]))\n",
        "\n",
        "# generate questions and false answers for each keywords\n",
        "for i in range(len(filtered_kw)):\n",
        "  for x in filtered_kw[i]:\n",
        "    results = false_answers(x)\n",
        "    if results != None:\n",
        "      questions_w_ans.append([gen_question(summary[i], x), x])\n",
        "      random.shuffle(results)\n",
        "      all_answers.append(results)"
      ],
      "metadata": {
        "id": "AdnLCwnhKN3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(questions_w_ans)):\n",
        "  print(questions_w_ans[i][0])\n",
        "  print(all_answers[i])"
      ],
      "metadata": {
        "id": "UWiGrp10LDeJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}