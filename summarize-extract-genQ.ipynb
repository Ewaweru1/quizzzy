{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abstractive-text-summarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8YQleV6xlIBiH5ppzLLNJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthick47v2/mcq-generator/blob/main/summarize-extract-genQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ71-vZKkARx"
      },
      "outputs": [],
      "source": [
        "# TODO : OPTIMIZEEEEEE\n",
        "# TODO : RUN MULTI TEXT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE BATCH OF TEXTS"
      ],
      "metadata": {
        "id": "0Xe73sHTopJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DONT FORGET TO CHANGE TO CUDA VERSION\n",
        "# !pip3 install torch==1.8.2+cpu torchvision==0.9.2+cpu torchaudio==0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip3 install transformers==4.15.0    # only this ver works with FastT5\n",
        "!pip3 install SentencePiece\n",
        "!pip3 install git+https://github.com/boudinfl/pke.git\n",
        "# FAST T5\n",
        "!pip3 install fastt5"
      ],
      "metadata": {
        "id": "Mc2Cy2Kskuzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pke\n",
        "import string\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-vZ_gitSlVZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Fu1t-Zv7TyIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from fastT5 import export_and_get_onnx_model, generate_onnx_representation, quantize\n",
        "# from transformers import T5Config, AutoTokenizer\n",
        "\n",
        "# t = 't5-base'\n",
        "\n",
        "# path = generate_onnx_representation(t)\n",
        "# q_path = quantize(path)\n",
        "# tokenizer_onnx = AutoTokenizer.from_pretrained(t)\n",
        "# conf = T5Config.from_pretrained(t)"
      ],
      "metadata": {
        "id": "AEOKy9guVdqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer_onnx.save_pretrained('models/')\n",
        "# conf.save_pretrained('models/')"
      ],
      "metadata": {
        "id": "3uoPYeIGWKGi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r models '/content/gdrive/MyDrive/mcq-gen/t5-summarize'"
      ],
      "metadata": {
        "id": "VS9Itv2zWiaM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastT5 import get_onnx_model, get_onnx_runtime_sessions, OnnxT5 \n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# summarize\n",
        "model_path = '/content/gdrive/MyDrive/mcq-gen/t5-summarize/t5-base'\n",
        "model_name = \"t5-base\"\n",
        "encoder_path = os.path.join(model_path, f\"{model_name}-encoder-quantized.onnx\")\n",
        "decoder_path = os.path.join(model_path, f\"{model_name}-decoder-quantized.onnx\")\n",
        "init_decoder_path = os.path.join(model_path, f\"{model_name}-init-decoder-quantized.onnx\")\n",
        "\n",
        "model_sessions = get_onnx_runtime_sessions((encoder_path,decoder_path,init_decoder_path))\n",
        "sum_model = OnnxT5(model_path, model_sessions)\n",
        "sum_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# que model\n",
        "model_path = '/content/gdrive/MyDrive/mcq-gen/t5-question'\n",
        "model_name = 't5_squad_v1'\n",
        "encoder_path = os.path.join(model_path, f\"{model_name}-encoder-quantized.onnx\")\n",
        "decoder_path = os.path.join(model_path, f\"{model_name}-decoder-quantized.onnx\")\n",
        "init_decoder_path = os.path.join(model_path, f\"{model_name}-init-decoder-quantized.onnx\")\n",
        "\n",
        "model_sessions = get_onnx_runtime_sessions((encoder_path,decoder_path,init_decoder_path))\n",
        "q_model = OnnxT5(model_path, model_sessions)\n",
        "q_tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "bTpj4BtffXsm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_splitted_text(text):\n",
        "  # https://huggingface.co/docs/transformers/preprocessing  - Tokenize\n",
        "  encode = sum_tokenizer.encode_plus(\"summarize: \" + text, return_tensors='pt', max_length=512, pad_to_max_length=False, truncation=True)\n",
        "  return encode[\"input_ids\"], encode[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "bO_R5WSmod0q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_bulk_text(text):\n",
        "  text = text.strip()\n",
        "  text = re.sub('[\\u2010-\\u2013]', '-', text)\n",
        "  text = re.sub('[^a-zA-Z0-9\\.,-?]', ' ', text)\n",
        "  text = re.sub(' {2,}', ' ', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "3v2ZAekno8bw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, range=300):\n",
        "  bulk_text = preprocess_bulk_text(text)\n",
        "  splitted_texts = []\n",
        "  while(len(bulk_text) > range):\n",
        "    i = range\n",
        "    while((i < len(bulk_text)) and (bulk_text[i] != '.')):\n",
        "      i += 1\n",
        "    splitted_texts.append(bulk_text[:(i+1)])\n",
        "    bulk_text = bulk_text.replace(bulk_text[:(i+1)], \"\")\n",
        "\n",
        "  return splitted_texts"
      ],
      "metadata": {
        "id": "qautz5BgIdL_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_summary(text):\n",
        "  output = \"\"\n",
        "\n",
        "  for x in sent_tokenize(text):\n",
        "    x = x.capitalize()\n",
        "    output += \" \" + x\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "tUkxLqEpxtvz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text):\n",
        "  input_tokens_ids, attention_mask = preprocess_splitted_text(text)\n",
        "  summary_encoded = sum_model.generate(input_ids=input_tokens_ids, \n",
        "                                   attention_mask=attention_mask,\n",
        "                                   num_beams=3,\n",
        "                                   num_return_sequences=1,\n",
        "                                   no_repeat_ngram_size=2, \n",
        "                                   max_length=300, \n",
        "                                   early_stopping=True)\n",
        "  # decode summarized token\n",
        "  output = sum_tokenizer.decode(summary_encoded[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  return postprocess_summary(output)"
      ],
      "metadata": {
        "id": "8LCkFuQeiBSq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(text, kw_pop):\n",
        "  ex = pke.unsupervised.MultipartiteRank()\n",
        "  ex.load_document(text)   \n",
        "\n",
        "  pos = {'PROPN','NOUN'}\n",
        "  stoplist = list(string.punctuation) \n",
        "  stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "  stoplist += stopwords.words('english')\n",
        "  ex.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "\n",
        "  ex.candidate_weighting(alpha=1.1, threshold=0.75, method='average')\n",
        "\n",
        "  kw = ex.get_n_best(n=kw_pop)\n",
        "\n",
        "  kw_ls = []\n",
        "\n",
        "  for i in kw:\n",
        "    kw_ls.append(i[0])\n",
        "\n",
        "  return kw_ls"
      ],
      "metadata": {
        "id": "4UJu2CGRpiBO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords(original, summarized, kw_pop=5):\n",
        "  orig_ls = extract_keywords(original, kw_pop)\n",
        "  sum_ls = extract_keywords(summarized, kw_pop)\n",
        "\n",
        "  orig_ls = set(orig_ls)\n",
        "  return list(orig_ls.intersection(sum_ls))"
      ],
      "metadata": {
        "id": "2vUS8makp5dG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_summary(context, answer):\n",
        "  text = \"context: {} answer: {}\".format(context, answer)\n",
        "  # https://huggingface.co/docs/transformers/preprocessing  - Tokenize\n",
        "  encode = q_tokenizer.encode_plus(text, return_tensors='pt', max_length = 382, pad_to_max_length=False, truncation=True)\n",
        "  return encode[\"input_ids\"], encode[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "XjZQKLdP6tHG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_question(context, answer):\n",
        "  input_tokens_ids, attention_mask = preprocess_summary(context, answer)\n",
        "\n",
        "  question_encoded = q_model.generate(input_ids=input_tokens_ids, \n",
        "                                             attention_mask=attention_mask,\n",
        "                                             num_beams=5,\n",
        "                                             no_repeat_ngram_size=2, \n",
        "                                             max_length=72, \n",
        "                                             early_stopping=True)\n",
        "  # decode summarized token\n",
        "  output = q_tokenizer.decode(question_encoded[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "  output = output.replace(\"question: \", \"\")\n",
        "  output = output.strip()\n",
        "  return output"
      ],
      "metadata": {
        "id": "vbrnFDvjRcT4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = \"\"\"\n",
        "Centralized DBMS architecture, It means files are stored in a single location and it is maintained from only that location. It is usually used by universities, companies and banks.\n",
        "# Advantages of Centralized DBMS architecture, \n",
        "# The data integrity is increased. So, the data is accurate and consistent.\n",
        "# Since all data in one place, data security is increased.\n",
        "# It is cheaper as it requires less power and maintenance.\n",
        "# Disadvantages of Centralized DBMS architecture,\n",
        "# Lot of data access traffic which may bottleneck in some situations.\n",
        "# If multiple users try to access it simultaneously this may reduce the efficiency of the system.\n",
        "# There is no database recovery measure when system failure occurs, so all data will be lost.\n",
        "# Since all data is centralized, it takes more time to access it and search. And if the network is slow, then it takes even more time.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E4DtgAa_W8Kc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitted_text = split_text(full_text)\n",
        "summary = []\n",
        "filtered_kw = []\n",
        "\n",
        "for i in range(len(splitted_text)):\n",
        "  summary.append(summarize(splitted_text[i]))\n",
        "  filtered_kw.append(filter_keywords(splitted_text[i], summary[i]))\n",
        "\n",
        "for i in range(len(filtered_kw)):\n",
        "  for x in filtered_kw[i]:\n",
        "    print(gen_question(splitted_text[i], x))\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "U8j7R21w1k-3",
        "outputId": "0e8d15de-e4f4-4b80-fc33-31d3221ff9f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Along with universities and banks, what organizations use Centralized DBMS?\n",
            "companies\n",
            "Along with universities and companies, what other institutions use Centralized DBMS?\n",
            "banks\n",
            "What is increased with Centralized DBMS?\n",
            "data integrity\n",
            "If multiple users try to access it simultaneously this may reduce the efficiency of what?\n",
            "system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1_77Ljek3RU-"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}